{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics of hateful articles\n",
    "\n",
    "En esta notebook veremos los tópicos de los artículos que generan cierto odio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from mongoengine import connect\n",
    "from hatespeech_models import Tweet, Article\n",
    "\n",
    "client = connect(\"hatespeech-labelling\")\n",
    "db = client[\"hatespeech-labelling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6669\n"
     ]
    }
   ],
   "source": [
    "\n",
    "initial_query = {\n",
    "    \"comments__19__exists\": True,\n",
    "}\n",
    "articles = Article.objects(**initial_query).as_pymongo()\n",
    "articles = list(articles)\n",
    "\n",
    "print(len(articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    hateful_comments = [c for c in article[\"comments\"] if c[\"hateful_value\"] > 0.5]\n",
    "    \n",
    "    article[\"num_hateful_comments\"] = len(hateful_comments)\n",
    "    article[\"avg_hateful_comments\"] = len(hateful_comments) / len(article[\"comments\"])\n",
    "    article[\"avg_hate_value\"] = sum(c[\"hateful_value\"] for c in article[\"comments\"]) / len(article[\"comments\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3520"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hateful_articles = [art for art in articles if art[\"avg_hateful_comments\"] > 0.13]\n",
    "len(hateful_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de - la - que - el - en - y - a - los - del - se - las - por - un - para - con - no - una - su - al - lo - como - más - pero - sus - le - ya - o - este - sí - porque - esta - entre - cuando - muy - sin - sobre - también - me - hasta - hay - donde - quien - desde - todo - nos - durante - todos - uno - les - ni - contra - otros - ese - eso - ante - ellos - e - esto - mí - antes - algunos - qué - unos - yo - otro - otras - otra - él - tanto - esa - estos - mucho - quienes - nada - muchos - cual - poco - ella - estar - estas - algunas - algo - nosotros - mi - mis - tú - te - ti - tu - tus - ellas - nosotras - vosotros - vosotras - os - mío - mía - míos - mías - tuyo - tuya - tuyos - tuyas - suyo - suya - suyos - suyas - nuestro - nuestra - nuestros - nuestras - vuestro - vuestra - vuestros - vuestras - esos - esas - estoy - estás - está - estamos - estáis - están - esté - estés - estemos - estéis - estén - estaré - estarás - estará - estaremos - estaréis - estarán - estaría - estarías - estaríamos - estaríais - estarían - estaba - estabas - estábamos - estabais - estaban - estuve - estuviste - estuvo - estuvimos - estuvisteis - estuvieron - estuviera - estuvieras - estuviéramos - estuvierais - estuvieran - estuviese - estuvieses - estuviésemos - estuvieseis - estuviesen - estando - estado - estada - estados - estadas - estad - he - has - ha - hemos - habéis - han - haya - hayas - hayamos - hayáis - hayan - habré - habrás - habrá - habremos - habréis - habrán - habría - habrías - habríamos - habríais - habrían - había - habías - habíamos - habíais - habían - hube - hubiste - hubo - hubimos - hubisteis - hubieron - hubiera - hubieras - hubiéramos - hubierais - hubieran - hubiese - hubieses - hubiésemos - hubieseis - hubiesen - habiendo - habido - habida - habidos - habidas - soy - eres - es - somos - sois - son - sea - seas - seamos - seáis - sean - seré - serás - será - seremos - seréis - serán - sería - serías - seríamos - seríais - serían - era - eras - éramos - erais - eran - fui - fuiste - fue - fuimos - fuisteis - fueron - fuera - fueras - fuéramos - fuerais - fueran - fuese - fueses - fuésemos - fueseis - fuesen - sintiendo - sentido - sentida - sentidos - sentidas - siente - sentid - tengo - tienes - tiene - tenemos - tenéis - tienen - tenga - tengas - tengamos - tengáis - tengan - tendré - tendrás - tendrá - tendremos - tendréis - tendrán - tendría - tendrías - tendríamos - tendríais - tendrían - tenía - tenías - teníamos - teníais - tenían - tuve - tuviste - tuvo - tuvimos - tuvisteis - tuvieron - tuviera - tuvieras - tuviéramos - tuvierais - tuvieran - tuviese - tuvieses - tuviésemos - tuvieseis - tuviesen - teniendo - tenido - tenida - tenidos - tenidas - tened'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('spanish')\n",
    "\" - \".join(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "l = list(nlp(\"Hola viejas de re mil mierdas, las odio profundamente desde mi corazón\"))\n",
    "\n",
    "lemmatized = [t.lemma_ for t in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe1cc6399884077b6472387c8bf96f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "lemmatized_data = []\n",
    "\n",
    "for art in tqdm(hateful_articles):\n",
    "    lemmatized_data.append([t.lemma_.lower() for t in nlp(art[\"body\"])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf8e0f0c6e2489f85bba432da78f3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def filter_words(text):\n",
    "    non_stop = [tok for tok in text if tok not in stop_words]\n",
    "    non_punct = [tok for tok in text if tok not in string.punctuation and \"\\n\" not in tok]\n",
    "    return non_punct\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for text in tqdm(lemmatized_data):\n",
    "    filtered_data.append(filter_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 3), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 2), (15, 2), (16, 2), (17, 1), (18, 3), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 2), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 10), (39, 1), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 4), (46, 4), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 1), (62, 1), (63, 5), (64, 1), (65, 2), (66, 1), (67, 1), (68, 1), (69, 1), (70, 4), (71, 1), (72, 12), (73, 1), (74, 1), (75, 2), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 14), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 2), (99, 1), (100, 1), (101, 4), (102, 48), (103, 1), (104, 5), (105, 1), (106, 1), (107, 1), (108, 8), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 3), (116, 1), (117, 3), (118, 1), (119, 32), (120, 1), (121, 1), (122, 1), (123, 1), (124, 30), (125, 2), (126, 1), (127, 1), (128, 1), (129, 3), (130, 1), (131, 2), (132, 1), (133, 1), (134, 2), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 3), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 2), (150, 1), (151, 10), (152, 1), (153, 1), (154, 1), (155, 2), (156, 1), (157, 4), (158, 2), (159, 2), (160, 1), (161, 1), (162, 1), (163, 3), (164, 1), (165, 4), (166, 1), (167, 1), (168, 2), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 46), (176, 9), (177, 1), (178, 2), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 5), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 2), (191, 2), (192, 1), (193, 2), (194, 3), (195, 5), (196, 2), (197, 1), (198, 3), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 3), (205, 2), (206, 4), (207, 1), (208, 1), (209, 2), (210, 3), (211, 3), (212, 1), (213, 1), (214, 1), (215, 3), (216, 3), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 2), (223, 1), (224, 1), (225, 7), (226, 3), (227, 1), (228, 1), (229, 2), (230, 5), (231, 3), (232, 1), (233, 1), (234, 1), (235, 1), (236, 14), (237, 1), (238, 3), (239, 1), (240, 1), (241, 1), (242, 4), (243, 1), (244, 1), (245, 4), (246, 3), (247, 1), (248, 11), (249, 1), (250, 1), (251, 10), (252, 5), (253, 2), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 3), (260, 2), (261, 1), (262, 1), (263, 15), (264, 5), (265, 1), (266, 10), (267, 1), (268, 12), (269, 1), (270, 2), (271, 2), (272, 3), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 2), (281, 2), (282, 3), (283, 2), (284, 1), (285, 1), (286, 1), (287, 2), (288, 1), (289, 1), (290, 1), (291, 3), (292, 8), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 2), (300, 1), (301, 1), (302, 23), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1), (308, 2), (309, 2)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(filtered_data)\n",
    "\n",
    "# Create Corpus\n",
    "texts = filtered_data\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=30, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6,\n",
      "  '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + '\n",
      "  '0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + '\n",
      "  '0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'),\n",
      " (0,\n",
      "  '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + '\n",
      "  '0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + '\n",
      "  '0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'),\n",
      " (29,\n",
      "  '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + '\n",
      "  '0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + '\n",
      "  '0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'),\n",
      " (10,\n",
      "  '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + '\n",
      "  '0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + '\n",
      "  '0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'),\n",
      " (13,\n",
      "  '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + '\n",
      "  '0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + '\n",
      "  '0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'),\n",
      " (14,\n",
      "  '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + '\n",
      "  '0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + '\n",
      "  '0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'),\n",
      " (28,\n",
      "  '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + '\n",
      "  '0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + '\n",
      "  '0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'),\n",
      " (7,\n",
      "  '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + '\n",
      "  '0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + '\n",
      "  '0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'),\n",
      " (27,\n",
      "  '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + '\n",
      "  '0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + '\n",
      "  '0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'),\n",
      " (11,\n",
      "  '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + '\n",
      "  '0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + '\n",
      "  '0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'),\n",
      " (5,\n",
      "  '0.072*\"lo\" + 0.063*\"de\" + 0.052*\"en\" + 0.029*\"y\" + 0.029*\"los\" + 0.019*\"a\" '\n",
      "  '+ 0.016*\"ciudad\" + 0.016*\"se\" + 0.016*\"el\" + 0.015*\"barrio\"'),\n",
      " (4,\n",
      "  '0.083*\"de\" + 0.061*\"lo\" + 0.044*\"el\" + 0.030*\"gustar\" + 0.025*\"me\" + '\n",
      "  '0.023*\"en\" + 0.019*\"twitter\" + 0.018*\"la\" + 0.016*\"facebook\" + '\n",
      "  '0.016*\"whatsapp\"'),\n",
      " (8,\n",
      "  '0.163*\"“\" + 0.163*\"”\" + 0.051*\"lo\" + 0.034*\"de\" + 0.034*\"el\" + 0.031*\"que\" '\n",
      "  '+ 0.022*\"y\" + 0.016*\"en\" + 0.016*\"a\" + 0.012*\"uno\"'),\n",
      " (26,\n",
      "  '0.093*\"de\" + 0.065*\"lo\" + 0.047*\"en\" + 0.034*\"el\" + 0.028*\"uno\" + 0.026*\"y\" '\n",
      "  '+ 0.026*\"que\" + 0.020*\"los\" + 0.020*\"ser\" + 0.018*\"a\"'),\n",
      " (2,\n",
      "  '0.060*\"de\" + 0.041*\"y\" + 0.039*\"su\" + 0.035*\"lo\" + 0.033*\"en\" + 0.029*\"a\" + '\n",
      "  '0.025*\"uno\" + 0.021*\"con\" + 0.021*\"me\" + 0.020*\"el\"'),\n",
      " (9,\n",
      "  '0.080*\"de\" + 0.079*\"lo\" + 0.048*\"el\" + 0.031*\"y\" + 0.029*\"en\" + 0.028*\"que\" '\n",
      "  '+ 0.021*\"a\" + 0.021*\"del\" + 0.016*\"uno\" + 0.015*\"por\"'),\n",
      " (25,\n",
      "  '0.080*\"de\" + 0.061*\"lo\" + 0.046*\"el\" + 0.038*\"en\" + 0.033*\"que\" + 0.025*\"y\" '\n",
      "  '+ 0.018*\"se\" + 0.016*\"a\" + 0.013*\"con\" + 0.013*\"los\"'),\n",
      " (21,\n",
      "  '0.095*\"de\" + 0.073*\"lo\" + 0.042*\"el\" + 0.035*\"en\" + 0.032*\"que\" + 0.030*\"y\" '\n",
      "  '+ 0.024*\"a\" + 0.022*\"uno\" + 0.018*\"del\" + 0.016*\"parir\"'),\n",
      " (12,\n",
      "  '0.099*\"de\" + 0.067*\"lo\" + 0.046*\"el\" + 0.037*\"en\" + 0.035*\"a\" + 0.032*\"que\" '\n",
      "  '+ 0.026*\"uno\" + 0.026*\"y\" + 0.020*\"por\" + 0.017*\"se\"'),\n",
      " (22,\n",
      "  '0.093*\"que\" + 0.046*\"no\" + 0.044*\"ser\" + 0.038*\"lo\" + 0.037*\"el\" + '\n",
      "  '0.031*\"a\" + 0.029*\"uno\" + 0.024*\"y\" + 0.022*\"tener\" + 0.020*\"estar\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + 0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + 0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'), (1, '0.076*\"de\" + 0.046*\"lo\" + 0.039*\"el\" + 0.039*\"en\" + 0.019*\"que\" + 0.016*\"país\" + 0.015*\"haber\" + 0.015*\"uno\" + 0.014*\"estados\" + 0.013*\"unidos\"'), (2, '0.051*\"de\" + 0.050*\"su\" + 0.034*\"lo\" + 0.033*\"a\" + 0.029*\"en\" + 0.029*\"y\" + 0.028*\"el\" + 0.019*\"me\" + 0.018*\"uno\" + 0.018*\"con\"'), (3, '0.134*\"tinelli\" + 0.088*\"marcelo\" + 0.031*\"guillermina\" + 0.031*\"cande\" + 0.023*\"candelaria\" + 0.022*\"valdés\" + 0.018*\"esquel\" + 0.017*\"showmatch\" + 0.009*\"valdes\" + 0.006*\"cordillera\"'), (4, '0.125*\"de\" + 0.075*\"lo\" + 0.043*\"en\" + 0.039*\"el\" + 0.028*\"y\" + 0.021*\"del\" + 0.018*\"los\" + 0.018*\"a\" + 0.018*\"se\" + 0.016*\"la\"'), (5, '0.077*\"de\" + 0.064*\"lo\" + 0.036*\"el\" + 0.030*\"a\" + 0.028*\"en\" + 0.024*\"que\" + 0.019*\"por\" + 0.019*\"uno\" + 0.015*\"y\" + 0.014*\"se\"'), (6, '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + 0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + 0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'), (7, '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + 0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + 0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'), (8, '0.084*\"lo\" + 0.084*\"de\" + 0.049*\"el\" + 0.043*\"que\" + 0.031*\"y\" + 0.029*\"en\" + 0.026*\"a\" + 0.020*\"uno\" + 0.019*\"del\" + 0.015*\"parir\"'), (9, '0.047*\"de\" + 0.047*\"lo\" + 0.045*\"el\" + 0.029*\"fernández\" + 0.026*\"presidente\" + 0.023*\"en\" + 0.021*\"y\" + 0.021*\"alberto\" + 0.021*\"con\" + 0.015*\"que\"'), (10, '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + 0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + 0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'), (11, '0.058*\"lo\" + 0.046*\"que\" + 0.043*\"el\" + 0.042*\"de\" + 0.031*\"en\" + 0.018*\"y\" + 0.017*\"se\" + 0.014*\"a\" + 0.012*\"ser\" + 0.012*\"salud\"'), (12, '0.088*\"de\" + 0.066*\"lo\" + 0.043*\"en\" + 0.037*\"y\" + 0.035*\"el\" + 0.032*\"que\" + 0.030*\"uno\" + 0.027*\"“\" + 0.027*\"”\" + 0.025*\"a\"'), (13, '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + 0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + 0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'), (14, '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + 0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + 0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'), (15, '0.088*\"que\" + 0.046*\"no\" + 0.042*\"ser\" + 0.033*\"el\" + 0.030*\"a\" + 0.030*\"lo\" + 0.027*\"uno\" + 0.024*\"y\" + 0.022*\"tener\" + 0.020*\"estar\"'), (16, '0.000*\"teleconferencia-\" + 0.000*\"compulsar\" + 0.000*\"xiaowei\" + 0.000*\"zhaoxu\" + 0.000*\"pelotero\" + 0.000*\"salta.-\" + 0.000*\"11:59\" + 0.000*\"hiperdesarrollo\" + 0.000*\"poder-\" + 0.000*\"prodemocracia\"'), (17, '0.017*\"canosa\" + 0.000*\"viviana\" + 0.000*\"elnueve\" + 0.000*\"borensztein\" + 0.000*\"pará\" + 0.000*\"laburen\" + 0.000*\"comentario\" + 0.000*\"exmarido\" + 0.000*\"peronista\" + 0.000*\"huevo\"'), (18, '0.073*\"de\" + 0.052*\"lo\" + 0.039*\"el\" + 0.031*\"en\" + 0.026*\"que\" + 0.021*\"a\" + 0.019*\"uno\" + 0.019*\"y\" + 0.016*\"del\" + 0.014*\"millón\"'), (19, '0.063*\"de\" + 0.039*\"el\" + 0.028*\"que\" + 0.028*\"en\" + 0.025*\"a\" + 0.024*\"lo\" + 0.017*\"y\" + 0.016*\"se\" + 0.014*\"médico\" + 0.013*\"con\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:sklearn not present, switch to PCoA\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type 'complex' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/hatespeech-labelling-i4Bxdr6F/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/hatespeech-labelling-i4Bxdr6F/lib/python3.6/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(data, kwds)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text/html'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     formatter.for_type(PreparedData,\n\u001b[0;32m--> 313\u001b[0;31m                        lambda data, kwds=kwargs: prepared_data_to_html(data, **kwds))\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/hatespeech-labelling-i4Bxdr6F/lib/python3.6/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[0;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[1;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                            \u001b[0mvis_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/hatespeech-labelling-i4Bxdr6F/lib/python3.6/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/hatespeech-labelling-i4Bxdr6F/lib/python3.6/site-packages/pyLDAvis/utils.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'complex' is not JSON serializable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=                        x                   y  topics  cluster       Freq\n",
       "topic                                                                    \n",
       "22    -0.395789+0.000000j  0.030037+0.000000j       1        1  18.877346\n",
       "12    -0.398210+0.000000j  0.004294+0.000000j       2        1  15.963403\n",
       "25    -0.381677+0.000000j  0.023587+0.000000j       3        1  14.850221\n",
       "21    -0.395913+0.000000j  0.013633+0.000000j       4        1  13.810974\n",
       "9     -0.400902+0.000000j  0.004682+0.000000j       5        1   9.255079\n",
       "2     -0.388636+0.000000j  0.033003+0.000000j       6        1   7.913988\n",
       "26    -0.385497+0.000000j  0.038253+0.000000j       7        1   7.448424\n",
       "4     -0.351758+0.000000j -0.006329+0.000000j       8        1   3.944589\n",
       "8     -0.349049+0.000000j  0.016838+0.000000j       9        1   3.796279\n",
       "5     -0.336327+0.000000j -0.015059+0.000000j      10        1   2.945794\n",
       "18     0.015992+0.000000j -0.433817+0.000000j      11        1   0.488607\n",
       "1      0.068484+0.000000j -0.020589+0.000000j      12        1   0.448253\n",
       "24     0.176129+0.000000j -0.001541+0.000000j      13        1   0.108050\n",
       "15     0.199349+0.000000j  0.026234+0.000000j      14        1   0.055934\n",
       "23     0.202760+0.000000j  0.027122+0.000000j      15        1   0.043899\n",
       "20     0.195715+0.000000j  0.024957+0.000000j      16        1   0.039476\n",
       "17     0.209456+0.000000j  0.017689+0.000000j      17        1   0.003340\n",
       "19     0.208913+0.000000j  0.016693+0.000000j      18        1   0.000508\n",
       "3      0.208913+0.000000j  0.016693+0.000000j      19        1   0.000508\n",
       "11     0.208913+0.000000j  0.016693+0.000000j      20        1   0.000495\n",
       "28     0.208913+0.000000j  0.016693+0.000000j      21        1   0.000490\n",
       "7      0.208913+0.000000j  0.016693+0.000000j      22        1   0.000489\n",
       "16     0.208913+0.000000j  0.016693+0.000000j      23        1   0.000486\n",
       "14     0.208913+0.000000j  0.016693+0.000000j      24        1   0.000486\n",
       "6      0.208913+0.000000j  0.016693+0.000000j      25        1   0.000485\n",
       "0      0.208913+0.000000j  0.016693+0.000000j      26        1   0.000485\n",
       "27     0.208913+0.000000j  0.016693+0.000000j      27        1   0.000482\n",
       "29     0.208913+0.000000j  0.016693+0.000000j      28        1   0.000479\n",
       "13     0.208913+0.000000j  0.016693+0.000000j      29        1   0.000477\n",
       "10     0.208913+0.000000j  0.016693+0.000000j      30        1   0.000475, topic_info=                     Term           Freq          Total Category  logprob  \\\n",
       "308                     “   15193.000000   15193.000000  Default  30.0000   \n",
       "309                     ”   15130.000000   15130.000000  Default  29.0000   \n",
       "102                    de  166944.000000  166944.000000  Default  28.0000   \n",
       "175                    lo  141395.000000  141395.000000  Default  27.0000   \n",
       "124                    en   75297.000000   75297.000000  Default  26.0000   \n",
       "...                   ...            ...            ...      ...      ...   \n",
       "14404  georreferenciación       0.000267       0.533148  Topic30 -10.6722   \n",
       "14403            disociar       0.000267       0.533148  Topic30 -10.6722   \n",
       "14402         cruzamiento       0.000267       0.533148  Topic30 -10.6722   \n",
       "14401           chaqueños       0.000267       0.533148  Topic30 -10.6722   \n",
       "14400                 248       0.000267       0.533148  Topic30 -10.6722   \n",
       "\n",
       "       loglift  \n",
       "308    30.0000  \n",
       "309    29.0000  \n",
       "102    28.0000  \n",
       "175    27.0000  \n",
       "124    26.0000  \n",
       "...        ...  \n",
       "14404   4.6583  \n",
       "14403   4.6583  \n",
       "14402   4.6583  \n",
       "14401   4.6583  \n",
       "14400   4.6583  \n",
       "\n",
       "[1881 rows x 6 columns], token_table=      Topic      Freq   Term\n",
       "term                        \n",
       "2679     12  0.998984     -a\n",
       "3373      1  0.989610    -la\n",
       "3374      1  0.995048    -no\n",
       "919       8  0.976763  00:00\n",
       "15        1  0.000992   2020\n",
       "...     ...       ...    ...\n",
       "309       6  0.010641      ”\n",
       "309       9  0.989327      ”\n",
       "677       8  0.999786      •\n",
       "2264      3  0.086056      …\n",
       "2264      6  0.913837      …\n",
       "\n",
       "[1921 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[23, 13, 26, 22, 10, 3, 27, 5, 9, 6, 19, 2, 25, 16, 24, 21, 18, 20, 4, 12, 29, 8, 17, 15, 7, 1, 28, 30, 14, 11])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds='mmds')\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type 'complex' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-0801cbc8fb4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/hatespeech-labelling-i4Bxdr6F/lib/python3.6/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(data, local, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd3_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ldavis_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ldavis_css_url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_ipynb_local_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_data_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m def show(data, ip='127.0.0.1', port=8888, n_retries=50,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/hatespeech-labelling-i4Bxdr6F/lib/python3.6/site-packages/pyLDAvis/_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[0;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[1;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                            \u001b[0mvis_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/hatespeech-labelling-i4Bxdr6F/lib/python3.6/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/hatespeech-labelling-i4Bxdr6F/lib/python3.6/site-packages/pyLDAvis/utils.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.10/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'complex' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
